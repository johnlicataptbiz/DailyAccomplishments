Daily Work Story Engine – Design Overview

Product Philosophy and Approach

Story First, Metrics Second: The engine prioritizes a compelling narrative as the primary means for a user to understand their day. Numbers serve to support the story, not overwhelm it. No single metric will define the day’s narrative – instead the system synthesizes multiple signals to capture the day’s truth[1]. For example, an excessive number of meetings might hurt deep focus time; rather than just reporting “5 meetings, 3 hours focus,” the story would explain how meetings fragmented the day’s flow, with metrics as evidence. Every inference or insight in the story must be defensible with data (no guessing the user’s intent or feelings). The tone is that of a sharp analyst examining the data, not a cheerleader or a scolding coach. This means focusing on facts, patterns, and implications in a neutral, professional tone.

Multi-Source Truth: The engine combines timeline data, focus metrics, meeting loads, context switches, and historical baselines to classify the day and derive insights. This holistic approach addresses the complexity of knowledge work that simple metrics miss[2]. For instance, traditional measures like “hours worked” or “emails sent” are inadequate on their own[2]. A Harvard Business Review study found that excessive messaging and collaboration can reduce productivity by up to 25%, underscoring the need to balance metrics like collaboration time with deep work time in any meaningful summary[1]. Therefore, the narrative synthesizes across all signals to present a balanced view.

Privacy by Default: The engine is designed to work even if only high-level, non-sensitive data is available. It does not rely on detailed task names or content. If certain qualitative inputs (like actual project names or detailed accomplishments) are missing, the story still functions by focusing on time distribution and generic categories. For example, instead of saying “Finished Project X,” it might say “completed a major task” if the data indicates a significant output or time block labeled as productive. This ensures personal privacy – the story remains high-level and pattern-focused.

Meaning Over Metrics: The narrative emphasizes interpretation and meaning. Rather than reciting “You had 4 deep work blocks and a 60% productivity score,” it might say: “You carved out multiple focus blocks (totaling 4 hours) primarily in the morning, which is higher than your usual deep focus – a sign of sustained effort on key work.” The goal is to translate metrics into insight: e.g., explaining that a long afternoon gap suggests an unusual interruption or break, or that a spike in context switches indicates a “firefighting” afternoon. Every data point used in the story answers “So what?” for the user. If it doesn’t add interpretive value, it’s left out.

Conceptual Architecture

The Daily Work Story Engine consists of modular components for data processing, insight extraction, narrative generation, and quality control. It is designed for production scalability – meaning it can handle many users’ data in parallel and degrade gracefully if AI components fail. Below is the high-level architecture with each stage:
 • Data Ingestion & Preprocessing: Collect structured productivity data from various sources (PC activity logs, calendar, app usage, etc.) for the day. Compute canonical signals such as deep work blocks, context switch count, focus durations, meeting hours, breaks, etc. These signals are computed by upstream systems (assumed available as input). The engine normalizes these inputs into a consistent internal format (e.g., JSON with fields like focus_blocks, meeting_time, context_switch_count, productivity_score, activity_timeline, etc.).
 /* Lines 10-11 omitted */
 • Insight Extraction Engine: This module analyzes the day’s data in context of historical baselines to derive key insights and classifications. It includes several sub-components:
 a. Day Classification: A rule-based or machine-learning classifier labels the day with a “Day Type” (see Day Type Taxonomy below). It examines patterns like focus distribution, fragmentation, meeting load, and compares them to user’s normal range. For example, if deep focus time is very high and meetings low, it might classify as a “Deep Focus Day.” If interruptions and context switches are rampant, it could be a “Fragmented Day.” If metrics are all average, perhaps a “Typical Day.” The classification provides the overarching theme for the narrative.
 b. Anomaly & Contrast Detection: The engine flags significant deviations or noteworthy patterns. It uses statistical baseline comparisons (e.g., if today’s focus hours are 2 standard deviations above the 4-week average, mark it as an anomaly). This prevents the story from treating every small fluctuation as newsworthy – only meaningful deviations are highlighted[3][1]. For instance, a 5% drop in productivity score might be normal variance (ignored), but a 30% drop with much higher context switching is significant (to be addressed in narrative). Multiple signals are cross-checked to see if they corroborate a story: e.g., a high context-switch count alongside short average focus blocks would consistently indicate fragmentation – reinforcing that insight. If signals conflict (e.g., high deep work hours and high context switches – a paradoxical scenario), the engine notes this as a point of uncertainty or complexity to mention.
 c. Insight Ranking: All extracted insights (e.g., “focus above baseline”, “many interruptions”, “long mid-day break”, “unusually high meeting load”, “completed notable task”) are ranked by importance. Importance is determined by factors like: deviation from baseline (larger deviations rank higher), impact on productivity score, and relevance to the day’s classified type. The highest-ranked insight (usually the day’s type or main story) will lead the narrative (forming the “headline” of the day), with supporting insights later. This hierarchy ensures the narrative has a clear focus and doesn’t bury the lede.
 • Narrative Synthesis (AI Generation): The top insights and day classification feed into a narrative generation module. This is powered by a Large Language Model (LLM) or advanced templating system. The system constructs a prompt to the LLM containing:
 /* Lines 16-19 omitted */
 • Requested output structure, such as: first an executive summary sentence, then a detailed paragraph, written in the third person or second person analytically.
 The LLM then produces a draft narrative text following these guidelines. The narrative focuses on “why” and “so what”, not just “what”. For example, rather than listing “You had 3 deep work sessions and 5 meetings”, it might generate: “Your morning was dominated by deep work (3 sessions totaling 4 hours), an unusually high focus that likely advanced key projects. Meetings were minimal (only 5), giving you more uninterrupted time than usual – a pattern reflected in above-average productivity.” Each statement is traced to the input data to avoid any fabrication. This use of provided facts is a form of retrieval augmentation – grounding the LLM in reality to prevent hallucinations[4].
 • Quality Control & Fallback Logic: The generated narrative is then checked by a post-processing module for accuracy and compliance. This module can employ business rules and even another model:
 /* Lines 22-26 omitted */
 • Delivery & Interaction: The final narrative is delivered to the user (e.g., in a daily email or dashboard). The system may also provide interactive elements: for instance, the narrative text might have tooltips or toggles to reveal the exact data behind a statement. For example, hovering “an unusually high focus” could show “4h vs 2.5h avg” as a tooltip. Users could be allowed to click to see a chart of their day. These affordances ensure the story is not a dead text – it’s an entry point to exploring one’s data, if desired. However, by default the narrative stands on its own, giving a clear overview without requiring any clicks.
 ScaIability Considerations: All heavy data crunching (like computing baselines, anomaly detection) is done asynchronously or in distributed fashion so that adding more users doesn’t slow down the system. The LLM generation might be the slowest step; to scale, the engine can batch requests or use fine-tuned smaller models for speed. Caching is used for static references like baseline comparisons (which don’t change daily). If the LLM service is down or rate-limited, the system automatically uses the template fallback for all users to ensure continuity of service.
 Day Type Taxonomy (Workday Classifications)
 One core output of the insight engine is a Day Type – a label that categorizes the workday’s dominant pattern. This classification anchors the narrative (“What kind of day was it?”). Below is a taxonomy of day types, including criteria and examples:
 • Deep Focus Day: Characterized by sustained concentration with minimal fragmentation. Criteria: long deep work blocks (e.g. 2+ hours at a stretch, and >50% of the day in focus)[5], few context switches, and fewer meetings than usual. Example: “You had a Deep Focus Day – with two major focus blocks in the morning and early afternoon totaling 5 hours. Meetings were under 1 hour, enabling an unusually uninterrupted workflow.” This is the “maker’s day” archetype – depth over breadth.
 /* Lines 31-37 omitted */
 • Focus-Breadth Mix Day: (Optional category) This describes days that blend depth and breadth – perhaps a deep focus morning and a fragmented afternoon. It’s essentially a hybrid, and might be captured by combining classifications if needed (e.g., “Focused Morning, Fragmented Afternoon”). In narrative we might not label it separately but describe the contrast within the day.
 Each Day Type has threshold rules – often based on comparisons to a user’s own baseline to personalize it. For example, “Meeting-Heavy” might be defined as “meeting hours > 1.5x your 4-week average” or “>50% of your total work time.” “Deep Focus Day” might require “focus time > 1.5x baseline and at least one block >2 hours”[5]. These thresholds can be tuned as more data is collected to ensure the classifications feel right. The system should also allow multiple tags if applicable, but typically one primary type is used to keep the story coherent. (If multiple patterns compete, the insight ranking will decide which to emphasize, while the other pattern is mentioned as a secondary aspect.)
 This taxonomy not only drives the narrative content (the story language differs for a Focus Day vs a Fragmented Day) but also sets user expectations. Over time, a user learns what a “Focus Day” means in this system’s terms. The story engine will include brief definitions in the UI or tooltips for these terms so users have clarity (for example, hover on “Deep Focus Day” might show “Mostly uninterrupted deep work, very few meetings or switches”).
 Insight Hierarchy and Conflict Resolution
 When multiple signals vie for attention, the engine must decide which insights lead and how to reconcile conflicts. The Insight Hierarchy is essentially a decision framework for prioritizing and framing information in the story:
 • Primary Insight (Lead): This usually relates to the Day Type classification or the most deviant aspect of the day. It’s the answer to “what stands out most about this day?” If, for instance, the day is a “Meeting-Heavy Day,” that fact will lead the narrative (“Today was dominated by meetings…”). If nothing is extreme, the primary insight might be more nuanced (“Today was a balanced, routine day with no major extremes, which can be good for steady progress.”). The primary insight sets the narrative’s theme – everything following will support or detail it.
 /* Lines 43-44 omitted */
 • Tertiary Details: Minor data points are either omitted or briefly noted toward the end. These include metrics that were normal/unexceptional or details that are interesting but not impactful. For instance, if the user’s “productivity score” (an aggregate metric) was, say, 7/10 and typical for them, the story might not mention it at all (to avoid noise). Or it might appear as “overall productivity was in line with your average” in a wrap-up sentence. Tertiary info can also be contextual, like “(This week, your focus time has been gradually rising each day)” – a detail connecting to a longer trend, if relevant.
 Resolving Conflicting Signals: Sometimes data sends mixed messages. High deep work hours and high context switching is one example – perhaps the user had two long focus blocks but the rest of the day was chaotic. Instead of ignoring one, the narrative will acknowledge the complexity. For example: “Interestingly, although you achieved 3.5 hours of deep work (above average), you also had an elevated number of context switches (25 vs your usual ~10). This suggests the day started focused but later became fragmented.” In the hierarchy, neither insight is dropped; one is framed as the primary (say the focus time, if that’s more above baseline), and the conflicting signal is the secondary explained as a contrasting element. Highlighting contrasts actually adds credibility, showing the system isn’t oversimplifying.
 The engine’s rules ensure that a big conflict triggers a contrast narrative structure. This often takes the form of “While X was high, Y was low” or “a tale of two halves: first part vs second part.” Using contrast not only communicates insight but also inherently provides an explanation (one likely caused the other, or the day was split in nature). High-quality analysis often uses such contrasts for insight[1]. For example, an intelligence brief might say “Enemy activity increased in the morning but dropped sharply by evening, indicating ...” Similarly, our story might say: “Your early day was quiet and focused, but after 2 PM an influx of meetings and task switches broke the flow, resulting in a fragmented afternoon.”
 Insight Suppression: If certain metrics are highly correlated or duplicative, the story won’t state both separately. E.g., “context switches were high” and “focus blocks were short” are basically the same phenomenon in different words – mentioning both can be redundant. The hierarchy logic would combine them: “Frequent context switching kept focus blocks short.” This respects the user’s attention by not repeating the same insight in two forms.
 When Everything is “Average”: If no metric stands out (a truly normal day), the engine still produces a narrative, but it will emphasize steadiness and perhaps a minor positive or neutral note. For instance: “Today was a typical workday with a healthy balance – no records broken, but steady progress. You had roughly your usual focus time (X hours) and a normal share of meetings. In short, it was a regular productive day without any dramatic swings.” This reassures the user that the system didn’t find anything alarm-worthy – which in itself is an insight (consistency can be good). It’s important the story doesn’t sound blank or say “nothing to report” – instead it validates that the day was within expectations.
 Contextualizing with History: Part of the hierarchy is relating insights to historical context. A high metric isn’t meaningful unless we know it’s high compared to what. So for each key metric insight, the narrative provides a comparison: either to the user’s average or to yesterday or a goal if available. For example, “4 hours of deep work” becomes “4 hours of deep work (about 1.5 hours above your usual)”. Using historical baselines transforms raw data into insight about performance[7]. This also helps identify anomalies (if it’s way off trend). The engine fetches a relevant baseline for each insight: e.g., a 4-week rolling average or a similar day-of-week average. If baseline data is sparse, it might use a default population benchmark or simply state the value without comparison.
 Manager vs Individual Perspective: The system can tailor the hierarchy slightly if the narrative is for the user themselves vs a manager summary. An individual’s story might highlight personal workflow issues (like context switching habits), whereas a manager-ready summary (which might be an input in the data) could emphasize outcomes and reliability (e.g., “completed key tasks, maintained consistent output”). The architecture can reuse the same insights but filter or rephrase them depending on audience. However, both share the same hierarchy logic – truthful, prioritized insights, just framed appropriately.

This workspace is in a dev container running on "Ubuntu 24.04.3 LTS".

Use `"$BROWSER" <url>` to open a webpage in the host's default browser.

Some of the command line tools available on the `PATH`: `apt`, `dpkg`, `docker`, `git`, `gh`, `kubectl`, `curl`, `wget`, `ssh`, `scp`, `rsync`, `gpg`, `ps`, `lsof`, `netstat`, `top`, `tree`, `find`, `grep`, `zip`, `unzip`, `tar`, `gzip`, `bzip2`, `xz`
